
#! /usr/bin/env python

import pickle;
import numpy as np;
from numpy.linalg import pinv;
from numpy.polynomial.legendre import legvander;
import cvxopt;
from cvxpy import Minimize, Problem, Variable,norm1,norm2,installed_solvers,lambda_max;
from cvxpy import norm as cvxpynorm;
import cvxpy;
# # #  Koopman optimization parameters # #  # 

poly_deg  =3;
continuous_predict=0;
use_cvx = 1;
quadratics = 1;
use_legendre = 0 ;

colors =[[ 0.68627453,  0.12156863,  0.16470589],
       [ 0.96862745,  0.84705883,  0.40000001],
       [ 0.83137256,  0.53333336,  0.6156863 ],
       [ 0.03529412,  0.01960784,  0.14509805],
       [ 0.90980393,  0.59607846,  0.78039217],
       [ 0.69803923,  0.87843138,  0.72941178],
       [ 0.20784314,  0.81568629,  0.89411765]];

colors = np.asarray(colors);
#np.random.randint(0,255,(Yf_final_test_ep.shape[0],3))
#colors = 1.0/255.0*np.asarray(colors,dtype=np.float32);
# # #  END # # # 

#file_path = 'koopman_data/rand_osc.pickle';
file_path = 'koopman_data/glycol.pickle';
#file_path='koopman_data/deltaomega-series.pickle';
#glycol.pickle');#deltaomega-series.pickle');


file_obj = file(file_path,'rb');
output_vec = pickle.load(file_obj);

Yp = output_vec[0]; # list of baseline observables, len(Yp) = (n_samps-1) 
Yf = output_vec[1]; # list of baseline observables, len(Yf) = (n_samps-1) 

if len(Yp)<2:
    print "Warning: the time-series data provided has no more than 2 points."
    

# not sure if we need this step. 
#for i in range(0,len(Yp)):
#    temp_Yp = np.asarray(Yp[i]);
#    temp_Yf = np.asarray(Yf[i]);
#    Yp[i] = temp_Yp.flatten();
#    Yf[i] = temp_Yf.flatten();
    
Y_whole = [None]*(len(Yp)+1);
        
for i in range(0,len(Yp)+1):
    if i == len(Yp):
        Y_whole[i] = Yf[i-1];
    else:
        Y_whole[i] = Yp[i];

Y_whole = np.asarray(Y_whole);
Y_whole = np.transpose(Y_whole);
print "[INFO] Number of total samples: " + repr(Y_whole.shape[1]);
print "[INFO] Observable dimension of a sample: " + repr(Y_whole.shape[0]);

# generate quadratic terms based on observables in Yp[i] for i = 1,.., n_samps
#output_nums = np.arange(0,len(Y_whole));
#import itertools;

#if quadratics:
#    max_combo_deg = 2;
#else:
#    max_combo_deg = 1;

def poly_lib(data,channels,poly):
    if poly == 1:
        return data;
    Yt_leg_stack = list(data)
    
    for i in range(channels):
        for j in range(i,channels):
            Yt_leg_stack.append(data[i,:]*data[j,:])
            #print(i,j)
            if poly > 2:
                for k in range (j,channels):
                    Yt_leg_stack.append(data[i,:]*data[j,:]*data[k,:])
                    #print(i,k,j)
                    if poly > 3:
                        for l in range (k,channels):
                            Yt_leg_stack.append(data[i,:]*data[j,:]*data[k,:]*data[l,:])
                            #print(i,k,j,l)
                            if poly > 4:
                                for m in range (l,channels):
                                    Yt_leg_stack.append(data[i,:]*data[j,:]*data[k,:]*data[l,:]*data[m,:])
                                    #print(i,k,j,l,m)
                        
    
    return np.asarray(Yt_leg_stack)


Y_whole = poly_lib(Y_whole,Y_whole.shape[0],poly_deg);

print "[INFO] Y_whole.shape after poly_lib: " + repr(Y_whole.shape);


#for curr_combo_deg in range(1,max_combo_deg+1):
#    all_pairwise_tuples =  list(itertools.combinations(output_nums,curr_combo_deg));
#    print "[DEBUG]: all_pairwise_tuples" + repr(all_pairwise_tuples);
#    for i in range(0,len(Y_whole)):
#        Y_wholetimepoint = Y_whole[i];
#        Y_append_all_combos = [];
#            
#        if curr_combo_deg==1:
#            Y_append = Y_wholetimepoint;
#        else:
#            Y_append = [None]*len(all_pairwise_tuples);
#            for j in range(0,len(all_pairwise_tuples)):
#                temp_tuple = all_pairwise_tuples[j];
#                Y_append[j] = 1.0;
#                for k in range(0,curr_combo_deg):
#                    n_k = temp_tuple[k];
#                    Y_append[j] = Y_append[j]*Y_wholetimepoint[n_k];
#        for elem in Y_append:
#            Y_append_all_combos.append(elem);

        #print Y_append_all_combos

#     Y_whole[i]= Y_append_all_combos;

Y_whole = np.asarray(Y_whole);
if quadratics:
    print "[INFO]: Quadratic lifted observable has the shape: " + repr(Y_whole.shape);
else:
    print "[INFO]: Quadratic lifting skipped, standard observable Y_whole has the shape: " + repr(Y_whole.shape);
Y_whole = (np.asarray(Y_whole));
#U,S,V = np.linalg.svd(Y_whole);

    #print S
#Y_whole_transformed = np.matmul(np.transpose(U),Y_whole);
#P = np.zeros((1,U.shape[0]));
#P[0][0] = 1.0;    
#Y_projected = np.matmul(P,Y_whole_transformed);
#print "projected dimension";
#print Y_projected.shape;
#print Y_projected;
Y_whole = np.transpose(Y_whole);
Yp = Y_whole[0:len(Y_whole)-1];
Yf = Y_whole[1:len(Y_whole)];

print "[INFO] Yp.shape (E-DMD): " + repr(Yp.shape);
print "[INFO] Yf.shape (E-DMD): " + repr(Yf.shape);




num_trains = len(Yp)*5/10;
train_indices = np.random.randint(0,len(Yp),num_trains)
test_indices = np.random.randint(0,len(Yp),len(Yp)-num_trains);

Yp_train = Yp[train_indices];
Yf_train = Yf[train_indices];
Yp_test = Yp[test_indices];
Yf_test = Yf[test_indices]; 


#train_range = len(Yp)*2/3; # define upper limits of training data 
#test_range = len(Yp); # define upper limits of test data 

#Yp_test = Yp[train_range:test_range];
#Yf_test = Yf[train_range:test_range];
#Yp_train = Yp[0:train_range];
#Yf_train = Yf[0:train_range];

#Yp_train = np.asarray(Yp_train);
#Yf_train = np.asarray(Yf_train);
#Yp_test = np.asarray(Yp_test);
#Yf_test = np.asarray(Yf_test);

print "[INFO] Yp_test.shape (E-DMD) " + repr(Yp_test.shape);
print "[INFO] Yf_test.shape (E-DMD) " + repr(Yf_test.shape);


Yp_final_train = Yp_train; # straight up quadratics training data 0,...,n-1 
Yf_final_train = Yf_train; # straight up quadratics training data 1 ,... n  1 step forward propagation of Yp_train
Yp_final_test = Yp_test; #straight up quadratics test data -
Yf_final_test = Yf_test; # straight up quadratics test data - 1-step forward propagation of Yp_test  

# # # - - - - - -  Legendre polynomial dictionary generation  - - - - -  # # # 

if use_legendre:
    
    #print Yp.shape
    deg_polynomial = poly_deg
    Yp_leg = legvander(Yp,deg_polynomial)
    n_points,n_channels,n_deg_polyp1=  Yp_leg.shape
    Yp_leg = np.reshape(Yp_leg,(n_deg_polyp1,n_points,n_channels));
    #print Yp_leg.shape

    Yf_leg = legvander(Yf,deg_polynomial)
    n_points,n_channels,n_deg_polyp1=  Yf_leg.shape
    Yf_leg = np.reshape(Yf_leg,(n_deg_polyp1,n_points,n_channels));
    #print Yf_leg.shape
    Yp_leg_stack = np.concatenate((Yp_leg[:,:,0],Yp_leg[:,:,1])) ;
    Yf_leg_stack = np.concatenate((Yf_leg[:,:,0],Yf_leg[:,:,1])) ;
    for i in range(2,n_channels):
        if i%1000==0:
                #print i
                #print Yp_leg_stack.shape
            print Yf_leg_stack.shape

        Yp_leg_stack = np.concatenate((Yp_leg_stack,Yp_leg[:,:,i]))
        Yf_leg_stack = np.concatenate((Yf_leg_stack,Yf_leg[:,:,i]))

        #print Yp_leg_stack.shape
        #print Yf_leg_stack.shape
    Yp_final_train = Yp_leg_stack;
    Yf_final_train = Yf_leg_stack;



    Yp_leg_test = legvander(Yp_test,deg_polynomial)
    n_points,n_channels,n_deg_polyp1=  Yp_leg_test.shape
    Yp_leg_test = np.reshape(Yp_leg_test,(n_deg_polyp1,n_points,n_channels));
    #print Yp_leg.shape

    Yf_leg_test = legvander(Yf_test,deg_polynomial)
    n_points,n_channels,n_deg_polyp1=  Yf_leg_test.shape
    Yf_leg_test = np.reshape(Yf_leg_test,(n_deg_polyp1,n_points,n_channels));
    #print Yf_leg_test.shape
    Yp_leg_stack_test = np.concatenate((Yp_leg_test[:,:,0],Yp_leg_test[:,:,1])) ;
    Yf_leg_stack_test = np.concatenate((Yf_leg_test[:,:,0],Yf_leg_test[:,:,1])) ;


    for i in range(2,n_channels):
        Yp_leg_stack_test = np.concatenate((Yp_leg_stack_test,Yp_leg_test[:,:,i]))
        Yf_leg_stack_test = np.concatenate((Yf_leg_stack_test,Yf_leg_test[:,:,i]))
    
    
    Yp_final_test = Yp_leg_stack_test;
    Yf_final_test = Yf_leg_stack_test;
    


# # # - - - - -  END Legendre polynomial dictionary generation - - - - -  


# # # - - - - -  Koopman calculation - - - - - # # # 

def calc_Koopman(Yf,Yp,flag=1):
    solver_instance = cvxpy.CVXOPT;
    #solver_instance = cvxpy.ECOS;
    if flag==1: # moore penrose inverse, plain ol' least squares Koopman
        #Yp_inv = np.dot(np.transpose(Yp_final), np.linalg.inv( np.dot(Yp_final,np.transpose(Yp_final)) )   );
        Yp_inv = np.linalg.pinv(Yp);
        K = np.dot(Yf,Yp_inv);
        

    if flag ==2: # cvx optimization approach - L2 + L1 lasso 
        norm1_term = 0.0;
        all_col_handles = [None]*Yf.shape[0]
        for i in range(0,Yf.shape[0]):            
            all_col_handles[i] = Variable(Yf.shape[0],1);
            norm1_term = norm1_term + norm2(all_col_handles[i]);

        operator = all_col_handles[0];
        for i in range(1,Yf.shape[0]):
            operator = cvxpy.hstack(operator,all_col_handles[i]);

        print "[INFO]: CVXPY Koopman operator variable: " +repr(operator);
        print "[INFO]: Yf.shape in calc_Koopman: " + repr(Yf.shape);
        norm2_fit_term = norm2(norm2(Yf-operator*Yp,axis=0));
        objective = Minimize(norm2_fit_term + norm1_term)
        constraints = [];
        prob = Problem(objective,constraints);
        result = prob.solve(verbose=True,solver=solver_instance);
        print "[INFO]: Finished executing cvx solver, printing CVXPY problem status"
        print(prob.status);
        K = operator.value;

    if flag ==3:
        operator = Variable(Yf.shape[0],Yf.shape[0])
        objective = Minimize(cvxpynorm(operator,2))
        constraints = [cvxpynorm(Yf-operator*Yp,'fro')/cvxpynorm(Yf,'fro')<0.01 ]
        prob = Problem(objective, constraints)
        result = prob.solve(verbose=True)#(solver=solver_instance);
        print(prob.status);
        K = operator.value;

    return K;



Yp_final_train = np.transpose(Yp_final_train);
Yf_final_train = np.transpose(Yf_final_train);
Yp_final_test = np.transpose(Yp_final_test);
Yf_final_test = np.transpose(Yf_final_test);


K = calc_Koopman(Yf_final_train,Yp_final_train,use_cvx); # use lsq Koopman 
Koopman_dim = K.shape[0]; # K should be square.

print "[INFO] Koopman_dim:" + repr(Koopman_dim);

if not( K.shape[1]==K.shape[0]):
    print "Warning! Estimated Koopman operator is not square with dimensions : " + repr(K.shape);


    
Yf_final_predicted = np.dot(K,Yp_final_train);


# # #  - - - - training error - - - # # # 
training_error =  np.linalg.norm(Yf_final_predicted-Yf_final_train,ord='fro')/np.linalg.norm(Yf_final_train,ord='fro');
print('%s%f' % ('[COMP] Training error: ',training_error));

# # # - - - - test error  - - - - # # #

Yf_final_test_predicted = np.dot(K,Yp_final_test);

test_error =  np.linalg.norm(Yf_final_test_predicted-Yf_final_test,ord='fro')/np.linalg.norm(Yf_final_test,ord='fro');
print('%s%f' % ('[COMP] Test error: ',test_error));

# # # - - - n-step prediction error - - - # # # 

n_points_pred = 

### Begin Sweep 

for n_depth_reciprocal in range(1,2):#max_depth-2): #2
  n_depth = max_depth+1 - n_depth_reciprocal;
  n_depth_control = max_depth_control + 1 - n_depth_reciprocal;   
  for min_width_conjugate in range(1,2):#min_width_limit): #2
      min_width= min_width_limit+1-min_width_conjugate;
      max_width = min_width; #zero gap between min and max ; use regularization and dropout to trim edges. 
      min_width_control = min_width_limit_control+1-min_width_conjugate;

      
      if min_width==max_width:
        hidden_vars_list = np.asarray([min_width]*n_depth);
      else:
        hidden_vars_list = np.random.randint(min_width,max_width,size=n_depth);

      if with_control:  
        hidden_vars_list_control = np.asarray([min_width_control]*n_depth_control);
        hidden_vars_list_control[-1] = deep_dict_size_control;

      good_start = 0;
      max_tries = 1;
      try_num = 0;
      
      # # # - - - enforce Koopman last layer # # #
      
      hidden_vars_list[-1] = deep_dict_size;
      
      print("[INFO] hidden_vars_list: ") +repr(hidden_vars_list);
      while good_start==0 and try_num < max_tries:
          try_num +=1;
          if debug_splash:
            print("\n Initialization attempt number: ") + repr(try_num);
            print("\n \t Initializing Tensorflow Residual ELU Network with ") + repr(n_inputs) + (" inputs and ") + repr(n_outputs) + (" outputs and ") + repr(len(hidden_vars_list)) + (" layers");

          with tf.device('/cpu:0'):
            Wy_list,by_list = initialize_Wblist(n_inputs,hidden_vars_list);
            params_list = [ n_outputs, deep_dict_size, hidden_vars_list,Wy_list,by_list,keep_prob,activation_flag, res_net ]
            
            psiypz_list,psiyp,yp_feed =  instantiate_comp_graph(params_list); 
            psiyfz_list,psiyf,yf_feed = instantiate_comp_graph(params_list);
            
            if with_control:
              Wu_list,bu_list = initialize_Wblist(n_inputs_control,hidden_vars_list_control);
              
              
              psiuz_list, psiu,u_control = initialize_stateinclusive_tensorflow_graph(n_inputs_control,deep_dict_size_control,hidden_vars_list_control,Wu_list,bu_list,keep_prob,activation_flag,res_net);
            
            # add hooks for affine control perturbation to Deep_Direct_Koopman_Objective
            step_size = tf.placeholder(tf.float32,shape=[]);

            if phase_space_stitching and (not with_control):
              try:
                Kmatrix_file_obj = file('phase_space_stitching/raws/Kmatrix_file.pickle','rb');
                this_pickle_file_list = pickle.load(Kmatrix_file_obj);
                Kx_num  = this_pickle_file_list[0];
                Kx = tf.constant(Kx_num); # this is assuming a row space (pre-multiplication) Koopman

              except:
                print("[Warning]: No phase space prior for the Koopman Matrix Detected @ /phase_space_stitching/raws/Kmatrix_file.pickle\n . . . learning Koopman prior as a fixed variable. ");
                no_phase_space_prior = True;
                Kx = weight_variable([deep_dict_size+n_outputs,deep_dict_size+n_outputs]);
            else: 
                Kx = weight_variable([deep_dict_size+n_outputs,deep_dict_size+n_outputs]);
            
            if with_control:
              Ku = weight_variable([deep_dict_size_control+n_inputs_control,deep_dict_size+n_outputs]);  # [NOTE: generalize to vary deep_dict_size (first dim, num of lifted inputs)             
              deep_koopman_loss,optimizer,forward_prediction_control = Deep_Control_Koopman_Objective(psiyp,psiyf,Kx,psiu,Ku,step_size);    
            else:
              deep_koopman_loss,optimizer,forward_prediction = Deep_Direct_Koopman_Objective(psiyp,psiyf,Kx,step_size,convex_basis=0,u=yp_feed);
              
            
            if debug_splash:
              train_vars = tf.trainable_variables();
              values = sess.run([x.name for x in train_vars]);
              print("[DEBUG] # of Trainable Variables: ") + repr(len(values));
              print("[DEBUG] Trainable Variables: ") + repr([ temp_var.shape for temp_var in values]);

              print("[DEBUG] # of datapoints in up_all_training: ") + repr(up_all_training.shape);
              print("[DEBUG] # of datapoints in uf_all_training: ") + repr(uf_all_training.shape);


            
            all_histories,good_start  = train_net(up_all_training,uf_all_training,deep_koopman_loss,optimizer,U_train,valid_error_threshold,test_error_threshold,max_iters,step_size_val);
            all_histories,good_start  = train_net(up_all_training,uf_all_training,deep_koopman_loss,optimizer,U_train,valid_error_threshold*.1,test_error_threshold*.1,max_iters,step_size_val/10);
            #all_histories,good_start  = train_net(up_all_training,uf_all_training,deep_koopman_loss,optimizer,U_train,valid_error_threshold*.025,test_error_threshold*.025,max_iters,step_size_val/100);

          training_error_history_nocovar = all_histories[0];
          validation_error_history_nocovar =   all_histories[1];
          test_error_history_nocovar = all_histories[2];
          training_error_history_withcovar  = all_histories[3];
          validation_error_history_withcovar = all_histories[4];
          test_error_history_withcovar = all_histories[5];
          print("[INFO] Initialization was successful: ") + repr(good_start==1);
          
          accuracy = deep_koopman_loss;#;
          if with_control:
            train_accuracy = accuracy.eval(feed_dict={yp_feed:up_all_training,yf_feed:uf_all_training,u_control:U_train});
            test_accuracy = accuracy.eval(feed_dict={yp_feed:Yp_test,yf_feed:Yf_test,u_control:U_test});
      
          else:
            train_accuracy = accuracy.eval(feed_dict={yp_feed:up_all_training,yf_feed:uf_all_training});
            test_accuracy = accuracy.eval(feed_dict={yp_feed:Yp_test,yf_feed:Yf_test});
      
          if test_accuracy <= best_test_error:
              best_test_error = test_accuracy;
              best_depth = n_depth;
              best_width = min_width;

              if debug_splash:
                print("[DEBUG]: Regularization penalty: ") + repr(sess.run(reg_term(Wy_list)));
              np.set_printoptions(precision=2,suppress=True);
              if debug_splash:
                print("[DEBUG]: ") + repr(np.asarray(sess.run(Wy_list[0]).tolist()));
          if debug_splash:
            print("[Result]: Training Error: ");
            print(train_accuracy);
            print("[Result]: Test Error : ");
            print(test_accuracy);
          
### Write Vars to Checkpoint Files/MetaFiles 
          
Kx_num = sess.run(Kx);

eig_val, eig_vec = np.linalg.eig(Kx_num) 

file_obj_swing = file('constrainedNN-Model.pickle','wb');
Wy_list_num = [sess.run(W_temp) for W_temp in Wy_list];
by_list_num = [sess.run(b_temp) for b_temp in by_list];

if with_control:
  Wu_list_num = [sess.run(W_temp) for W_temp in Wu_list];
  bu_list_num = [sess.run(b_temp) for b_temp in bu_list];
  Ku_num = sess.run(Ku);
  pickle.dump([Wy_list_num,by_list_num,Wu_list_num,bu_list_num,Kx_num,Ku_num],file_obj_swing);
else:
    pickle.dump([Wy_list_num,by_list_num,Kx_num],file_obj_swing);
file_obj_swing.close();


if (not phase_space_stitching) and (not with_control):
  file_obj_phase = file('phase_space_stitching/raws/Kmatrix_file.pickle','wb');
  pickle.dump([Kx_num],file_obj_phase);
  file_obj_phase.close();

saver = tf.train.Saver()

tf.add_to_collection('psiyp',psiyp);
tf.add_to_collection('psiyf',psiyf);
tf.add_to_collection('Kx',Kx);


if with_control:
  tf.add_to_collection('forward_prediction_control',forward_prediction_control);
  tf.add_to_collection('psiu',psiu);
  tf.add_to_collection('u_control',u_control);
  tf.add_to_collection('Ku',Ku);
else:
  tf.add_to_collection('forward_prediction',forward_prediction);

tf.add_to_collection('yp_feed',yp_feed);
tf.add_to_collection('yf_feed',yf_feed);

save_path = saver.save(sess, data_suffix + '.ckpt')

Koopman_dim = Kx_num.shape[0];
print("[INFO] Koopman_dim:") + repr(Kx_num.shape);

if single_series:
  if pre_examples_switch ==3:
     Y_p_old,Y_f_old,Y_whole,u_control_all_training = load_pickle_data('koopman_data/zhang_control.pickle');
  if pre_examples_switch == 4:
     Y_p_old,Y_f_old,Y_whole,u_control_all_training = load_pickle_data('koopman_data/deltaomega-singleseries.pickle'); 


  if not( Kx_num.shape[1]==Kx_num.shape[0]):
      print("Warning! Estimated Koopman operator is not square with dimensions : ") + repr(Kx_num.shape);

  train_range = len(Y_p_old)/2; # define upper limits of training data 

  if debug_splash:
    print("[DEBUG] train_range: ") + repr(train_range);

  #print("Y_p_old.shape") + repr(Y_p_old.shape);
  #print("Y_f_old.shape") + repr(Y_f_old.shape);


  test_range = len(Y_p_old); # define upper limits of test data 
  #Yp_test = Yp[0:test_range];
  #Yf_test = Yf[0:test_range];

  print("[DEBUG] test_range: ") + repr(test_range);
  Yp_test = Y_p_old[train_range:test_range];
  Yf_test = Y_f_old[train_range:test_range];

  if with_control:
    U_test = u_control_all_training_old[train_range:test_range];
    U_train = u_control_all_training_old[0:train_range];
    U_train = np.asarray(U_train);
    U_test = np.asarray(U_test);
    if len(U_test.shape)==1:
      U_train = np.reshape(U_train,(U_train.shape[0],1));
      U_test = np.reshape(U_test,(U_test.shape[0],1));


Yp_train = np.asarray(Yp_train);
Yf_train = np.asarray(Yf_train);
Yp_test = np.asarray(Yp_test);
Yf_test = np.asarray(Yf_test);
Yp_final_test = Yp_test;
Yf_final_test = Yf_test;
Yp_final_train = Yp_train;    
Yf_final_train = Yf_train;


# # # Print Evaluation Metrics -  Deep Koopman Learning # # #

#print("[DEBUG]: Yp_train.shape") + repr(Yp_train.shape);

if with_control:
  training_error = accuracy.eval(feed_dict={yp_feed:list(Yp_train),yf_feed:list(Yf_train),u_control:list(U_train)});
  test_error = accuracy.eval(feed_dict={yp_feed:list(Yp_test),yf_feed:list(Yf_test),u_control:list(U_test)});
else:
  training_error = accuracy.eval(feed_dict={yp_feed:list(Yp_train),yf_feed:list(Yf_train)});
  test_error = accuracy.eval(feed_dict={yp_feed:list(Yp_test),yf_feed:list(Yf_test)});
  
print('%s%f' % ('[COMP] Training error: ',training_error));
print('%s%f' % ('[COMP] Test error: ',test_error));

# # # - - - n-step Prediction Error Analysis - - - # # # 

  
n_points_pred = len(Y_p) - test_indices[0]-1;
init_index = test_indices[0];
print "[INFO] Yp_final_test.shape: " + repr(Yp_final_test.shape);
Ycurr = np.asarray(Y_p).T[:,init_index]
Yf_final_test_stack = np.asarray(Y_p).T[:,init_index:(init_index+1)+n_points_pred]
#Yp_final_test[:,init_index]; # Yf_leg_stack[:,-1]; # grab one time point from the final prediction of the training data

Yf_final_test_ep = [];


K = np.asarray(K);
print np.linalg.eig(K)

Yf_final_test_ep.append(Ycurr); # append the initial seed state value.


for i in range(0,n_points_pred):
    if continuous_predict:
        Ycurr = Ycurr;
    else:
            Ycurr = np.dot(K,Ycurr);
    
    Yf_final_test_ep.append(Ycurr);


Yf_final_test_ep = np.asarray(Yf_final_test_ep);
print "[INFO] Ground truth Yf_final_test_ep.shape:S1 " + repr(Yf_final_test_ep.shape);

Yf_final_test_ep = np.transpose(Yf_final_test_ep);

#Yf_final_test_stack = np.zeros((Yf_final_test.shape[0],Yf_final_test.shape[1]+1));
#for i in range(-1,Yf_final_test.shape[1]):
#    if i==-1:
#        Yf_final_test_stack[:,i+1] = Yp_final_test[:,0];
#    else:
#        Yf_final_test_stack[:,i+1] = Yf_final_test[:,i];
        
print "[INFO] Ground truth Yf_final_test_ep_stack.shape: "+ repr( Yf_final_test_stack.shape);
print "[INFO] Ground truth Yf_final_test_ep.shape: "+ repr(Yf_final_test_ep.shape); 
prediction_error = np.linalg.norm(Yf_final_test_stack-Yf_final_test_ep,ord='fro')/np.linalg.norm(Yf_final_test_stack,ord='fro');
print('%s%f' % ('[COMP] n-step Prediction error: ',prediction_error));

import matplotlib;
import matplotlib.pyplot as plt;

print "[DEBUG] Y_final_test_stack.shape: " + repr(Yf_final_test_stack.shape);
x_range = np.arange(0,Yf_final_test_stack.shape[1],1);




for i in range(0,4):
    plt.plot(x_range,Yf_final_test_ep[i,:],'--',color=colors[i,:])
    plt.plot(x_range,Yf_final_test_stack[i,:],'.',color=colors[i,:]);
#plt.plot(x_range,Yf_final_test_stack[0,:],'r-',label='x_1(t)');
#plt.plot(x_range,Yf_final_test_ep[1,:],'g*--',label='x_2_pred(t)');
#plt.plot(x_range,Yf_final_test_stack[1,:],'g-',label='x_2(t)');

plt.legend(loc='best');
#plt.xlabel('t');
import matplotlib
matplotlib.rcParams.update({'font.size':22})
plt.show();

stats_vec = [training_error,test_error,prediction_error];
# # #  - - - End n-step prediction error - - - # # # 

#file_obj = open('stats_file.txt','a');
#file_obj.write('\n%d\t%d\t%d\t%d\t%d\t%f\t%f\t%f' %(train_range,test_range,deg_polynomial,n_channels,Koopman_dim,training_error,test_error,prediction_error));
#file_obj.close();
#temp_stats = [train_range,test_range,deg_polynomial,n_channels,Koopman_dim,training_error,test_error,prediction_error];
#all_stats.append(temp_stats);#

#file_obj = open('stats_pickle.txt','wb');
#pickle.dump(all_stats,file_obj);
#file_obj.close()
